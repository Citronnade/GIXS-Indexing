
@article{habershon_powder_2004,
	title = {Powder Diffraction Indexing as a Pattern Recognition Problem: A New Approach for Unit Cell Determination Based on an Artificial Neural Network},
	volume = {108},
	issn = {1089-5639, 1520-5215},
	url = {https://pubs.acs.org/doi/10.1021/jp0310596},
	doi = {10.1021/jp0310596},
	shorttitle = {Powder Diffraction Indexing as a Pattern Recognition Problem},
	pages = {711--716},
	number = {5},
	journal = {The Journal of Physical Chemistry A},
	author = {Habershon, Scott and Cheung, Eugene Y. and Harris, Kenneth D. M. and Johnston, Roy L.},
	year = {2004},
	urldate = {2019-02-28},
	date = {2004-02},
	langid = {english},
	file = {Habershon et al. - 2004 - Powder Diffraction Indexing as a Pattern Recogniti.pdf:/home/max/Zotero/storage/6MJN54BM/Habershon et al. - 2004 - Powder Diffraction Indexing as a Pattern Recogniti.pdf:application/pdf}
}


@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
                  journal = {International Conference on Learning Representations, 2015},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2019-02-28},
	date = {2014-12-22},
	year = {2014},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/home/max/Zotero/storage/MLF3X72W/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/max/Zotero/storage/FA6A4RTR/1412.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://arxiv.org/abs/1502.03167},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	journal = {{arXiv}:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2019-02-28},
	date = {2015-02-10},
	year = {2015},
	eprinttype = {arxiv},
	eprint = {1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1502.03167 PDF:/home/max/Zotero/storage/WZKEBUJW/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/home/max/Zotero/storage/I6UA3CUG/1502.html:text/html}
}